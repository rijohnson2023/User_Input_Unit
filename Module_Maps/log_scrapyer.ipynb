{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rought plan\n",
    "\n",
    "1. Set up path to place files\n",
    "2. get the source_directory to search for the files\n",
    "3. os.walk() to find the files\n",
    "4. open the file\n",
    "   1. move pointer to the end of file if not using \"a+\" mode\n",
    "   2. normal termination line\n",
    "      1. if no normal termination line break and move on to next file\n",
    "   3. move pointer to the top of the file\n",
    "   4. iterate using readline\n",
    "      1. startswith(\"%\"), grab line\n",
    "      2. startswith(\"#\") grab line and all lines after until reach \"------\" line\n",
    "   5. end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locate/setup directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Riley/Documents/Research/User_Input_Module/LogFile\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def set_data_path() -> str:\n",
    "    outfile_name = input(\"Name the file to store the data:\\n(Do not include the extension)\\n\")\n",
    "\n",
    "    # Locate current working directory as destination for report\n",
    "    data_path = os.path.join(os.getcwd(),outfile_name)\n",
    "\n",
    "    while True :\n",
    "        # Two answers: acceptable: 1. path, 2. \"\", all else are bad\"\n",
    "        answer = str(input(\"The data recorded will be at path:\\n%10s\\nHit enter to proceed, or type new path without '/LogData.csv':\\n  \" % data_path))\n",
    "        if answer == \"\" :\n",
    "            break\n",
    "        elif os.path.isdir(answer) :\n",
    "            data_path = os.path.join(answer,outfile_name)\n",
    "            break\n",
    "\n",
    "    return data_path\n",
    "\n",
    "def test() :\n",
    "    \"\"\"\n",
    "    Simple data file string is:\n",
    "            LogFile\n",
    "\n",
    "    Copy and past path:\n",
    "            /Users/Riley/Documents/Research/User_Input_Module/\n",
    "    \"\"\"\n",
    "    data_path = set_data_path()\n",
    "    print(data_path)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 log files were found in the directory branch:\n",
      "/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/\n",
      "['/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChlorideClTS.log',\n",
      " '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/NotNormalTerm.log',\n",
      " '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/EmptyFile.log',\n",
      " '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/Pd2p-ene-CO-transcis-P-fromciscis.log',\n",
      " '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/Cl.log',\n",
      " '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChloride.log']\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "def find_files_in_dir() -> list[str] :\n",
    "    # Get valid source directory from user\n",
    "    source_dir = input(\"Please input the directory path to search:\\n\")\n",
    "    while not os.path.isdir(source_dir) :\n",
    "        source_dir = input(\"Invalid path, enter a new directory path to search:\\n\")\n",
    "\n",
    "    # Create a list of all the log files in the directory\n",
    "    listoffiles = list()\n",
    "    for (dirpath, dir_names, filenames) in os.walk(source_dir):\n",
    "        listoffiles += [os.path.join(dirpath,file) for file in filenames if file.split(\".\")[len(file.split(\".\"))-1] == \"log\"]\n",
    "\n",
    "    print(\"%d log files were found in the directory branch:\\n%s\" % (len(listoffiles), source_dir))\n",
    "    return listoffiles\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    Use example files in the directory at the path:\n",
    "            /Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/\n",
    "    \"\"\"\n",
    "    file_list = find_files_in_dir()\n",
    "    pp.pprint(file_list)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling the file challenge\n",
    "open the file\n",
    "   1. move pointer to the end of file if not using \"a+\" mode\n",
    "   2. normal termination line\n",
    "      1. if no normal termination line break and move on to next file\n",
    "   3. move pointer to the top of the file\n",
    "   4. iterate using readline\n",
    "      1. startswith(\"%\"), grab line\n",
    "      2. startswith(\"#\") grab line and all lines after until reach \"------\" line\n",
    "   5. end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File, AllylChloride.log, terminated normally.\n",
      "File, AllylChlorideClTS.log, terminated normally.\n",
      "Normal termination not detected in NotNormalTerm.log.\n",
      "File, EmptyFile.log, is empty.\n",
      "    File:\n",
      "     /Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChloride.log\n",
      "     Normal termination: True\n",
      "    File:\n",
      "     /Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChlorideClTS.log\n",
      "     Normal termination: True\n",
      "    File:\n",
      "     /Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/NotNormalTerm.log\n",
      "     Normal termination: False\n",
      "    File:\n",
      "     /Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/EmptyFile.log\n",
      "     Normal termination: False\n"
     ]
    }
   ],
   "source": [
    "from asyncore import file_wrapper\n",
    "\n",
    "def check_log_file(file_path: str, file_obj: file_wrapper) -> bool:\n",
    "    \"\"\"\n",
    "    Check log file searches the file object for the line\n",
    "    \"Normal termination of Gaussian 16\"\n",
    "\n",
    "    This is usually a substring of the last line in the log file, if it terminates correctly.\n",
    "    Sometimes with multistep calculations, there will be a \"Normal termination of Gaussian 16\"\n",
    "    line for every calculation step, or some may contain a \"Error termination of Gaussian 16.\"\n",
    "\n",
    "    We only want to record the keywords from the files that have terminated normally.\n",
    "    Those keywords are deemed as valid by the Gaussian 16 software.\n",
    "    Therefore, the log file must pass this check if we are going to count its keywords.\n",
    "\n",
    "    This function iterates over the bytes/characters of the file in reverse order until it finds the line\n",
    "    \"Normal termination of Gaussian 16.\" Keep in mind that if the file contains multi-step calculations,\n",
    "    and one of those calculations finished successfully, then the keywords will be counted.\n",
    "    \"\"\"\n",
    "    check = True\n",
    "    empty = False\n",
    "\n",
    "    file_name = (file_path.split(\"/\"))[len(file_path.split(\"/\"))-1]\n",
    "\n",
    "    # Check if size of file is 0\n",
    "    if os.stat(file_path).st_size == 0 :\n",
    "        empty = True\n",
    "        check = False\n",
    "\n",
    "    if empty != True :\n",
    "        # Move the cursor to the end of the file\n",
    "        file_obj.seek(0, os.SEEK_END)\n",
    "\n",
    "        # Get the current position of the pointer\n",
    "        pointer_location = file_obj.tell()\n",
    "        \n",
    "        # Create a buffer to keep the last read line\n",
    "        buffer = bytearray()\n",
    "\n",
    "        #Loop in reverse order until \"Normal termination\" line is found\n",
    "        while pointer_location >= 0 :\n",
    "            # Move the file pointer to the location pointed by the pointer\n",
    "            file_obj.seek(pointer_location)\n",
    "\n",
    "            pointer_location -= 1 # Shift pointer location back by 1\n",
    "            \n",
    "            new_byte = file_obj.read(1) # Read that byte/character\n",
    "\n",
    "            if new_byte == b'\\n' : # If the byte is a newline character, then the end of a line has been reached\n",
    "                line = buffer.decode()[::-1] #Fetch the line from buffer\n",
    "                if 'Normal termination of Gaussian 16' in line :\n",
    "                    print(\"File, %s, terminated normally.\" % file_name)\n",
    "                    break\n",
    "                # Reinitialize the byte array to save next line\n",
    "                buffer = bytearray()\n",
    "            else:\n",
    "                buffer.extend(new_byte)\n",
    "\n",
    "        # As file is read completely, if the line has not been detected then the file did not terminate normally.\n",
    "        if pointer_location <= 0:\n",
    "            print(\"Normal termination not detected in %s.\" %file_name)\n",
    "            check = False\n",
    "    else:\n",
    "        print(\"File, %s, is empty.\" % file_name)\n",
    "\n",
    "    return check\n",
    "\n",
    "def test():\n",
    "\n",
    "    # file_path1 is a perfectly completed calculation file\n",
    "    file_path1 = '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChloride.log'\n",
    "    file_path2 = '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChlorideClTS.log'\n",
    "\n",
    "    # file_path2 is a multi-step calculation log file. Both calculations terminated with error\n",
    "    file_path3 = '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/NotNormalTerm.log'\n",
    "    # file_path3 is an empty file with a log extension\n",
    "    file_path4 = '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/EmptyFile.log'\n",
    "    file_list = [file_path1,file_path2,file_path3, file_path4]\n",
    "    \n",
    "    checks = list()\n",
    "    for file_path in file_list :\n",
    "        with open(file_path,\"rb\") as file_obj :\n",
    "            checks.append(check_log_file(file_path,file_obj))\n",
    "    \n",
    "    [print(\"    File:\\n     %s\\n     Normal termination: %s\" %(file,checks[index])) for index, file in enumerate(file_list)]\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the data from the log file\n",
    "\n",
    "> The file has already been read as a binary file\n",
    "> \n",
    ">   - This was to check the last lines to see if it contained \"Normal termination of Gaussian 16\"\n",
    "> \n",
    ">   - There really isn't a need to read binary, unless that is required for reading the last line.\n",
    "> \n",
    "> For extracting the keywords, intuitively, it could also be read character by character. The difficulty would be making sure the characters don't get combined all together\n",
    "> \n",
    "> It seems easier to read by line. \n",
    "> \n",
    ">   - The challenge of reading line by line is if the Route section spills over into two lines, then its possible one of the keywords will get messed up. \n",
    "> \n",
    ">   - This multiplies one keyword messed up for every line spilled over. So for every line spilled over there are two bad inputs. \n",
    "> \n",
    ">   - Ideally, there won't be that many, so these will be relatively low scoring keywords when counting them as popular data.\n",
    "> \n",
    "> The next task is to compile all of the keywords into a dictionary that counts them. Then to sort the dictionary by the values, and write it into an outfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File, AllylChloride.log, terminated normally.\n",
      "['%chk=AllylChloride.chk', '%nprocs=60', '%mem=60GB']\n",
      "['opt=(maxcycles=85)', 'freq=noraman', 'hf/3-21g', 'scrf=(smd,solvent=dmso)', 'geo']\n",
      "\n",
      "File, AllylChlorideClTS.log, terminated normally.\n",
      "['%oldchk=AllylChlorideClTS.chk', '%chk=AllylChlorideClTS-TS.chk']\n",
      "['opt=(ts,noeigen,readfc,modredundant)', 'freq', 'hf/3-21g', 'scrf=(smd,solvent', 'scrf=(smd,solvent=dmso)', '=dmso)', 'nosymm', 'geom=checkpoint', 'guess=read']\n",
      "\n",
      "Normal termination not detected in NotNormalTerm.log.\n",
      "File, EmptyFile.log, is empty.\n"
     ]
    }
   ],
   "source": [
    "from asyncore import file_wrapper\n",
    "\n",
    "def extract_data(file_obj: file_wrapper) -> tuple[list[str]]:\n",
    "    # Initialize local var\n",
    "    link0 = bytearray()\n",
    "    route_cmds = bytearray()\n",
    "    extracted = False\n",
    "\n",
    "    # Move the cursor to the beginning of the file\n",
    "    file_obj.seek(0)\n",
    "\n",
    "    # Read first line\n",
    "    line = file_obj.readline()\n",
    "    while line:\n",
    "        # if the line starts with a %, it is a link0 command\n",
    "        if b\"%\" in line[:3] :\n",
    "                #print(\"Found link0 input\\n%s\" % line)\n",
    "                line = line.split()\n",
    "                [link0.extend(b\" \" + elem) for elem in line if b\"%\" in elem and elem != b\"\"]\n",
    "        # if the line starts with a #, it's the route section line\n",
    "        elif b\"#\" in line[:3] :\n",
    "            #print(\"Found Route Line\\n%s\" % line)\n",
    "            line = line.split()\n",
    "            [route_cmds.extend(b\" \" + elem) for elem in line if b\"#\" not in elem and elem != b\"\"]\n",
    "            # Grab the last keyword in case line spills over\n",
    "            end_keyword = line[len(line) - 1]\n",
    "            # grab next line if the contents spill over\n",
    "            line = file_obj.readline()\n",
    "            while b'----------------------------------------------------------------------' not in line :\n",
    "                line = line.split()\n",
    "                # Chances are if the line spills it wrecks a keyword. This takes the last keyword\n",
    "                # and combines it with the first of the next line\n",
    "                if len(line) > 1 :\n",
    "                    start_keyword = line[0]\n",
    "                    # Add the joined keyword\n",
    "                else :\n",
    "                    # Extra protection against bad lines\n",
    "                    break\n",
    "                route_cmds.extend(b\" \"+end_keyword+start_keyword)\n",
    "                # Extend bytearray by the additional keywords\n",
    "                [route_cmds.extend(b\" \" + elem) for elem in line if elem != b\"\"]\n",
    "                # Read the next line\n",
    "                line = file_obj.readline()\n",
    "            extracted = True\n",
    "        \n",
    "        # Once the route section commands are collected, stop iterating over the lines in the file.\n",
    "        if extracted == True:\n",
    "            break\n",
    "        # Iterate over the next line until the route section is collected\n",
    "        line = file_obj.readline()\n",
    "\n",
    "    return link0.decode(\"ascii\").split(), route_cmds.decode(\"ascii\").split()\n",
    "\n",
    "\n",
    "def test():\n",
    "    # file_path1 is a perfectly completed calculation file\n",
    "    file_path1 = '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChloride.log'\n",
    "    file_path2 = '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChlorideClTS.log'\n",
    "\n",
    "    # file_path2 is a multi-step calculation log file. Both calculations terminated with error\n",
    "    file_path3 = '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/NotNormalTerm.log'\n",
    "    # file_path3 is an empty file with a log extension\n",
    "    file_path4 = '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/EmptyFile.log'\n",
    "    file_list = [file_path1,file_path2,file_path3, file_path4]\n",
    "\n",
    "    for file_path in file_list :\n",
    "        with open(file_path,\"rb\") as infile :\n",
    "            check = check_log_file(file_path=file_path,file_obj=infile)\n",
    "            if check : \n",
    "                (link0, route_cmds) = extract_data(file_obj=infile)\n",
    "                print(link0)\n",
    "                print(route_cmds)\n",
    "                print()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'%chk=AllylChloride.chk': 1,\n",
      " '%chk=AllylChlorideClTS-TS.chk': 1,\n",
      " '%mem=60GB': 1,\n",
      " '%nprocs=60': 1,\n",
      " '%oldchk=AllylChlorideClTS.chk': 1}\n",
      "{}\n",
      "{'=dmso)': 1,\n",
      " 'freq': 1,\n",
      " 'freq=noraman': 1,\n",
      " 'geo': 1,\n",
      " 'geom=checkpoint': 1,\n",
      " 'geom=connectivity': 1,\n",
      " 'guess=read': 1,\n",
      " 'hf/3-21g': 1,\n",
      " 'm=connectivity': 1,\n",
      " 'nosymm': 1,\n",
      " 'opt=(maxcycles=85)': 1,\n",
      " 'opt=(ts,noeigen,readfc,modredundant)': 1,\n",
      " 'scrf=(smd,solvent': 1,\n",
      " 'scrf=(smd,solvent=dmso)': 1}\n"
     ]
    }
   ],
   "source": [
    "def update_mostFreq_data(mostFreq_dict:dict[str:int], keywords:list[str]) -> None:\n",
    "    \"\"\" \n",
    "    This function updates the mostFreq data used to track the most popular G16 keywords:\n",
    "    Update the count if the keyword is in the dictionary.\n",
    "    If the keyword is not in the dictionary, its added with a default count of 1.\n",
    "    \"\"\"\n",
    "    [mostFreq_dict.update({elem:mostFreq_dict[elem]+1}) for elem in keywords if elem in mostFreq_dict.keys()]\n",
    "    [mostFreq_dict.setdefault(elem,1) for elem in keywords if elem not in mostFreq_dict.keys()]\n",
    "\n",
    "def test():\n",
    "    # Data output by the extract data function\n",
    "    link0 = ['%chk=AllylChloride.chk', '%nprocs=60', '%mem=60GB'] + ['%oldchk=AllylChlorideClTS.chk', '%chk=AllylChlorideClTS-TS.chk']\n",
    "    route_cmds = ['opt=(maxcycles=85)', 'freq=noraman', 'hf/3-21g', 'scrf=(smd,solvent=dmso)', 'geo', 'm=connectivity', 'geom=connectivity'] + ['opt=(ts,noeigen,readfc,modredundant)', 'freq', 'hf/3-21g', 'scrf=(smd,solvent', '=dmso)', 'nosymm', 'geom=checkpoint', 'guess=read', 'scrf=(smd,solvent=dmso)']\n",
    "\n",
    "    # Initializing variables for the data. These need to be either global variables or class variables, \n",
    "    # so all the files update the same dictionary\n",
    "    link0_mostFreq = dict()\n",
    "    route_mostFreq = dict()\n",
    "\n",
    "    # Example link0 keyword update\n",
    "    pp.pprint(link0_mostFreq)\n",
    "    update_mostFreq_data(link0_mostFreq,link0)\n",
    "    pp.pprint(link0_mostFreq)\n",
    "    \n",
    "    # Example Route Section Keyword Update\n",
    "    pp.pprint(route_mostFreq)\n",
    "    update_mostFreq_data(route_mostFreq,route_cmds)\n",
    "    pp.pprint(route_mostFreq)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopping place:\n",
    "\n",
    "1. I can find all the files\n",
    "2. I can check which ones terminated normally\n",
    "3. I can extract the right data\n",
    "4. I can store that data in dictionaries where the key is the keyword and the integer is the times it shows up\n",
    "\n",
    "# To Do\n",
    "1. Sort the data\n",
    "2. Store it in a json file\n",
    "3. Store it in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AllylChloride.log\n",
      "['%chk=__filename__.chk']\n",
      "AllylChlorideClTS.log\n",
      "['%oldchk=__filename__.chk', '%chk=__filename__-TS.chk']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChloride.log\\n    File, AllylChloride.log, terminated normally.\\n    ['%chk=AllylChloride.chk', '%nprocs=60', '%mem=60GB']\\n\\n/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChlorideClTS.log\\n    File, AllylChlorideClTS.log, terminated normally.\\n    ['%oldchk=AllylChlorideClTS.chk', '%chk=AllylChlorideClTS-TS.chk']\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "def replace_filename(link0_list:list,file_path:str) -> list:\n",
    "    file_str = file_path.split(\"/\")[len(file_path.split(\"/\"))-1].replace(\".log\",\"\")\n",
    "    edited_link0 = [keyword.replace(file_str,\"__filename__\") for keyword in link0_list if file_str in keyword]\n",
    "    return edited_link0\n",
    "\n",
    "def test() :\n",
    "    file_list = ['/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChloride.log', '/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChlorideClTS.log']\n",
    "    link0_list = [['%chk=AllylChloride.chk', '%nprocs=60', '%mem=60GB'],['%oldchk=AllylChlorideClTS.chk', '%chk=AllylChlorideClTS-TS.chk']]\n",
    "    for file_path in file_list :\n",
    "        index = file_list.index(file_path)\n",
    "        edited_link0 = replace_filename(link0_list[index],file_path)\n",
    "        print(file_path.split(\"/\")[len(file_path.split(\"/\"))-1])\n",
    "        pp.pprint(edited_link0)\n",
    "\n",
    "\n",
    "test()\n",
    "\n",
    "\"\"\"\n",
    "/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChloride.log\n",
    "    File, AllylChloride.log, terminated normally.\n",
    "    ['%chk=AllylChloride.chk', '%nprocs=60', '%mem=60GB']\n",
    "\n",
    "/Users/Riley/Documents/Research/User_Input_Module/Gaussian_Outputs_Keywords/AllylChlorideClTS.log\n",
    "    File, AllylChlorideClTS.log, terminated normally.\n",
    "    ['%oldchk=AllylChlorideClTS.chk', '%chk=AllylChlorideClTS-TS.chk']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'=dmso)': 1,\n",
      " 'freq': 5,\n",
      " 'freq=noraman': 4,\n",
      " 'geo': 1,\n",
      " 'geom=checkpoint': 2,\n",
      " 'geom=connectivity': 1,\n",
      " 'guess=read': 6,\n",
      " 'hf/3-21g': 7,\n",
      " 'm=connectivity': 1,\n",
      " 'nosymm': 8,\n",
      " 'opt=(maxcycles=85)': 20,\n",
      " 'opt=(ts,noeigen,readfc,modredundant)': 2,\n",
      " 'scrf=(smd,solvent': 1,\n",
      " 'scrf=(smd,solvent=dmso)': 15}\n",
      "\n",
      "[('opt=(maxcycles=85)', 20),\n",
      " ('scrf=(smd,solvent=dmso)', 15),\n",
      " ('nosymm', 8),\n",
      " ('hf/3-21g', 7),\n",
      " ('guess=read', 6),\n",
      " ('freq', 5),\n",
      " ('freq=noraman', 4),\n",
      " ('geom=checkpoint', 2),\n",
      " ('opt=(ts,noeigen,readfc,modredundant)', 2),\n",
      " ('=dmso)', 1),\n",
      " ('geo', 1),\n",
      " ('geom=connectivity', 1),\n",
      " ('m=connectivity', 1),\n",
      " ('scrf=(smd,solvent', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Sort the data by the integer value from greatest to least\n",
    "def order_dict(mostFreq_dict:dict[str:int]) -> tuple[str,int]:\n",
    "    return sorted(mostFreq_dict.items(),key=lambda elem: elem[1],reverse=True)\n",
    "\n",
    "def sorted_dict(mostFreq_dict:dict[str:int]) -> dict[str:int]:\n",
    "    return dict(sorted(mostFreq_dict.items(),key=lambda elem: elem[1],reverse=True))\n",
    "\n",
    "def test():\n",
    "    route_mostFreq =    {'=dmso)': 1,\n",
    "                        'freq': 5,\n",
    "                        'freq=noraman': 4,\n",
    "                        'geo': 1,\n",
    "                        'geom=checkpoint': 2,\n",
    "                        'geom=connectivity': 1,\n",
    "                        'guess=read': 6,\n",
    "                        'hf/3-21g': 7,\n",
    "                        'm=connectivity': 1,\n",
    "                        'nosymm': 8,\n",
    "                        'opt=(maxcycles=85)': 20,\n",
    "                        'opt=(ts,noeigen,readfc,modredundant)': 2,\n",
    "                        'scrf=(smd,solvent': 1,\n",
    "                        'scrf=(smd,solvent=dmso)': 15}\n",
    "    route_mostFreq = sorted_dict(route_mostFreq)\n",
    "    pp.pprint(route_mostFreq)\n",
    "    print()\n",
    "    route_mostFreq = order_dict(route_mostFreq)\n",
    "    pp.pprint(route_mostFreq)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'%chk=AllylChloride.chk': 1,\n",
      " '%chk=AllylChlorideClTS-TS.chk': 1,\n",
      " '%mem=60GB': 15,\n",
      " '%nprocs=60': 15,\n",
      " '%oldchk=AllylChlorideClTS.chk': 1}\n",
      "{'=dmso)': 1,\n",
      " 'freq': 5,\n",
      " 'freq=noraman': 4,\n",
      " 'geo': 1,\n",
      " 'geom=checkpoint': 2,\n",
      " 'geom=connectivity': 1,\n",
      " 'guess=read': 6,\n",
      " 'hf/3-21g': 7,\n",
      " 'm=connectivity': 1,\n",
      " 'nosymm': 8,\n",
      " 'opt=(maxcycles=85)': 20,\n",
      " 'opt=(ts,noeigen,readfc,modredundant)': 2,\n",
      " 'scrf=(smd,solvent': 1,\n",
      " 'scrf=(smd,solvent=dmso)': 15}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def export_data_json(data_path: str, link0_mostFreq: dict[str:int]=None,route_mostFreq: dict[str:int]=None) -> str :\n",
    "    # Add the json extension to data_path\n",
    "    data_path += \".json\"\n",
    "    # Organize the data into a list of dicts\n",
    "    data = [link0_mostFreq,route_mostFreq]\n",
    "    # Write the list of dicts to the json file\n",
    "    with open(data_path,\"w\") as file_obj :\n",
    "        json.dump(data,file_obj)\n",
    "    return data_path\n",
    "\n",
    "def test() :\n",
    "    data_path = '/Users/Riley/Documents/Research/User_Input_Module/LogFile'\n",
    "\n",
    "    link0_mostFreq =    {'%nprocs=60': 15,\n",
    "                        '%mem=60GB': 15,\n",
    "                        '%chk=AllylChloride.chk': 1,\n",
    "                        '%chk=AllylChlorideClTS-TS.chk': 1,\n",
    "                        '%oldchk=AllylChlorideClTS.chk': 1}\n",
    "\n",
    "    route_mostFreq =    {'opt=(maxcycles=85)': 20,\n",
    "                        'scrf=(smd,solvent=dmso)': 15,\n",
    "                        'nosymm': 8,\n",
    "                        'hf/3-21g': 7,\n",
    "                        'guess=read': 6,\n",
    "                        'freq': 5,\n",
    "                        'freq=noraman': 4,\n",
    "                        'geom=checkpoint': 2,\n",
    "                        'opt=(ts,noeigen,readfc,modredundant)': 2,\n",
    "                        '=dmso)': 1,\n",
    "                        'geo': 1,\n",
    "                        'geom=connectivity': 1,\n",
    "                        'm=connectivity': 1,\n",
    "                        'scrf=(smd,solvent': 1}\n",
    "\n",
    "    data_path = export_data_json(data_path,link0_mostFreq,route_mostFreq)\n",
    "\n",
    "    # Wiping the variables\n",
    "    link0_mostFreq = None\n",
    "    route_mostFreq = None\n",
    "\n",
    "    # Example of importing the data from the json file\n",
    "    with open(data_path,\"r\") as file_obj :\n",
    "        [link0_mostFreq,route_mostFreq] = json.load(file_obj)\n",
    "\n",
    "    pp.pprint(link0_mostFreq)\n",
    "    pp.pprint(route_mostFreq)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'%chk=AllylChloride.chk': 1,\n",
      " '%chk=AllylChlorideClTS-TS.chk': 1,\n",
      " '%mem=60GB': 15,\n",
      " '%nprocs=60': 15,\n",
      " '%oldchk=AllylChlorideClTS.chk': 1}\n",
      "{'=dmso)': 1,\n",
      " 'freq': 5,\n",
      " 'freq=noraman': 4,\n",
      " 'geo': 1,\n",
      " 'geom=checkpoint': 2,\n",
      " 'geom=connectivity': 1,\n",
      " 'guess=read': 6,\n",
      " 'hf/3-21g': 7,\n",
      " 'm=connectivity': 1,\n",
      " 'nosymm': 8,\n",
      " 'opt=(maxcycles=85)': 20,\n",
      " 'opt=(ts,noeigen,readfc,modredundant)': 2,\n",
      " 'scrf=(smd,solvent': 1,\n",
      " 'scrf=(smd,solvent=dmso)': 15}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def export_data_csv(data_path: str, link0_mostFreq: tuple[str,int]=None,route_mostFreq: tuple[str,int]=None) -> str :\n",
    "    # Add the json extension to data_path\n",
    "    data_path += \".csv\"\n",
    "    link0_mostFreq = dict()\n",
    "    # Organize the data into a list of dicts\n",
    "    data = [(key,item) for key,item in link0_mostFreq.items()]\n",
    "    # Write the list of dicts to the json file\n",
    "    with open(data_path,\"w\") as file_obj :\n",
    "        json.dump(data,file_obj)\n",
    "    return data_path\n",
    "\n",
    "def test() :\n",
    "    data_path = '/Users/Riley/Documents/Research/User_Input_Module/LogFile'\n",
    "\n",
    "    link0_mostFreq =    {'%nprocs=60': 15,\n",
    "                        '%mem=60GB': 15,\n",
    "                        '%chk=AllylChloride.chk': 1,\n",
    "                        '%chk=AllylChlorideClTS-TS.chk': 1,\n",
    "                        '%oldchk=AllylChlorideClTS.chk': 1}\n",
    "\n",
    "    route_mostFreq =    {'opt=(maxcycles=85)': 20,\n",
    "                        'scrf=(smd,solvent=dmso)': 15,\n",
    "                        'nosymm': 8,\n",
    "                        'hf/3-21g': 7,\n",
    "                        'guess=read': 6,\n",
    "                        'freq': 5,\n",
    "                        'freq=noraman': 4,\n",
    "                        'geom=checkpoint': 2,\n",
    "                        'opt=(ts,noeigen,readfc,modredundant)': 2,\n",
    "                        '=dmso)': 1,\n",
    "                        'geo': 1,\n",
    "                        'geom=connectivity': 1,\n",
    "                        'm=connectivity': 1,\n",
    "                        'scrf=(smd,solvent': 1}\n",
    "\n",
    "    data_path = export_data_json(data_path,link0_mostFreq,route_mostFreq)\n",
    "\n",
    "    # Wiping the variables\n",
    "    link0_mostFreq = None\n",
    "    route_mostFreq = None\n",
    "\n",
    "    # Example of importing the data from the json file\n",
    "    with open(data_path,\"r\") as file_obj :\n",
    "        [link0_mostFreq,route_mostFreq] = json.load(file_obj)\n",
    "\n",
    "    pp.pprint(link0_mostFreq)\n",
    "    pp.pprint(route_mostFreq)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing all the units together\n",
    "\n",
    "1. set_data_path()\n",
    "2. find_files_in_dir()\n",
    "3. for file_path in file_list\n",
    "   1. check_log_file()\n",
    "   2. extract_data()\n",
    "   3. update_mostFreq_data()\n",
    "4. sort_dict()\n",
    "5. export_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'%chk=__filename__-TS.chk': 1,\n",
      " '%chk=__filename__.chk': 3,\n",
      " '%oldchk=__filename__.chk': 1}\n",
      "{'=dmso)': 1,\n",
      " 'b3lyp/gen': 1,\n",
      " 'freq': 2,\n",
      " 'freq=noraman': 2,\n",
      " 'geo': 1,\n",
      " 'geom=checkpoint': 1,\n",
      " 'geom=connectivity': 2,\n",
      " 'guess=read': 1,\n",
      " 'hf/3-21g': 3,\n",
      " 'nos': 1,\n",
      " 'nosymm': 2,\n",
      " 'opt=(maxcycles=85)': 1,\n",
      " 'opt=(ts,noeigen,readfc,modredundant)': 1,\n",
      " 'opt=maxcycles=85': 1,\n",
      " 'pseudo=read': 1,\n",
      " 'scrf=(smd,solvent': 1,\n",
      " 'scrf=(smd,solvent=dmso)': 3,\n",
      " 'scrf=(smd,solvent=water)': 1,\n",
      " 'ymm': 1}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint as pp\n",
    "# Read in and check the ExtractTest.json file\n",
    "data_path = '/Users/Riley/Documents/Research/User_Input_Module/ExtractData.json'\n",
    "\n",
    "# Example of importing the data from the json file\n",
    "with open(data_path,\"r\") as file_obj :\n",
    "    [link0_mostFreq,route_mostFreq] = json.load(file_obj)\n",
    "\n",
    "pp.pprint(link0_mostFreq)\n",
    "pp.pprint(route_mostFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "\n",
    "1. Need a clean up function for the %chk and %oldchk to substitute the filename with `__filename__`\n",
    "2. Need a version to write out to csv file\n",
    "3. Extract Data has rough patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "219f28c729eda709583e1ab80762c396fbb24857f282d49252dc64048c611cb8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('MOD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
